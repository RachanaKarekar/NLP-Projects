{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis_CNN_Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5NzLHqOXuLSbu0i7YmET8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoYP9C5dq9_V",
        "colab_type": "text"
      },
      "source": [
        "## Data used: http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
        "\n",
        "---\n",
        "The\n",
        "dataset is comprised of 1,000 positive and 1,000 negative movie reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps6T-k80lNZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_EH6LCrvdp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiabd_vGkcmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1mOMNg_9nYG",
        "colab_type": "text"
      },
      "source": [
        "# Store the Vocabulary\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bumZefyklLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "\n",
        "  # close the file\n",
        "  file.close()\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens\n",
        "\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\n",
        "  # load doc\n",
        "  doc = load_doc(filename)\n",
        "\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "\n",
        "  # update counts\n",
        "  vocab.update(tokens)  \n",
        "\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # add doc to vocab\n",
        "    add_doc_to_vocab(path, vocab)\n",
        "\n",
        "\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename, 'w')\n",
        "  file.write(data)  \n",
        "  file.close()   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvhVMACduEYK",
        "colab_type": "code",
        "outputId": "06e5ceba-b3d9-4501-c057-b5ba978ffc55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "# define vocab\n",
        "vocab = Counter()\n",
        "\n",
        "# add all docs to vocab\n",
        "process_docs(\"/content/gdrive/My Drive/NLP_Projects/4/txt_sentoken/neg\", vocab)\n",
        "process_docs(\"/content/gdrive/My Drive/NLP_Projects/4/txt_sentoken/pos\", vocab)\n",
        "\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))\n",
        "\n",
        "# keep tokens with > 2 occurrence\n",
        "min_occurence = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurence]\n",
        "print(len(tokens))\n",
        "\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, \"/content/gdrive/My Drive/NLP_Projects/4/vocab.txt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46557\n",
            "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n",
            "27139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqUK9QhZkpT6",
        "colab_type": "text"
      },
      "source": [
        "# Splitting the data as train and test\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MvARAoA9CSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  documents = list()\n",
        "\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "\n",
        "    # load the doc\n",
        "    doc = load_doc(path)\n",
        "\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc, vocab)\n",
        "\n",
        "    # add to list\n",
        "    documents.append(tokens)\n",
        "\n",
        "  return documents\n",
        "\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # filter out tokens not in vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  tokens = ' '.join(tokens)\n",
        "  return tokens  \n",
        "\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "  # load documents\n",
        "  neg = process_docs(\"/content/gdrive/My Drive/NLP_Projects/4/txt_sentoken/neg\", vocab, is_train)\n",
        "  pos = process_docs(\"/content/gdrive/My Drive/NLP_Projects/4/txt_sentoken/pos\", vocab, is_train)\n",
        "  docs = neg + pos\n",
        "\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "\n",
        "  return docs, labels\n",
        "\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer  \n",
        "\n",
        "\n",
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "  # integer encode\n",
        "  encoded = tokenizer.texts_to_sequences(docs)\n",
        "\n",
        "  # pad sequences\n",
        "  padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "\n",
        "  return padded   \n",
        "\n",
        "\n",
        "# define the model\n",
        "def define_model(vocab_size, max_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "  model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # compile network\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # summarize defined model\n",
        "  model.summary()\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\n",
        "  return model  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arBBhfWHa-L4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "outputId": "d1f171d0-8bd6-4337-e2ab-924550489105"
      },
      "source": [
        "# load the vocabulary\n",
        "vocab_filename = \"/content/gdrive/My Drive/NLP_Projects/4/vocab.txt\"\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load training data\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print('Maximum length: %d' % max_length)\n",
        "# encode data\n",
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
        "# define model\n",
        "model = define_model(vocab_size, max_length)\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "# save the model\n",
        "model.save(\"/content/gdrive/My Drive/NLP_Projects/4/model.h5\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 26897\n",
            "Maximum length: 1319\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1319, 100)         2689700   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 1312, 32)          25632     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 656, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 20992)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                209930    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 2,925,273\n",
            "Trainable params: 2,925,273\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " - 7s - loss: 0.6935 - accuracy: 0.4972\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.6347 - accuracy: 0.6406\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2011 - accuracy: 0.9511\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0118 - accuracy: 0.9989\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0048 - accuracy: 0.9989\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0025 - accuracy: 0.9994\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0019 - accuracy: 0.9994\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0012 - accuracy: 0.9994\n",
            "Epoch 10/10\n",
            " - 0s - loss: 6.5587e-04 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7PisVSRptIF",
        "colab_type": "text"
      },
      "source": [
        "# Predicting Sentiment for New Reviews and Evaluating the model\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlQbuU-HdnmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
        "  # clean review\n",
        "  line = clean_doc(review, vocab)\n",
        "\n",
        "  # encode and pad review\n",
        "  padded = encode_docs(tokenizer, max_length, [line])\n",
        "\n",
        "  # predict sentiment\n",
        "  yhat = model.predict(padded, verbose=0)\n",
        "\n",
        "  # retrieve predicted percentage and label\n",
        "  percent_pos = yhat[0,0]\n",
        "\n",
        "  if round(percent_pos) == 0:\n",
        "    return (1-percent_pos), 'NEGATIVE'\n",
        "\n",
        "  return percent_pos, 'POSITIVE'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1QUGKJ3bYXB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "2a1484c1-e43d-4a2b-f480-0cd74303ae48"
      },
      "source": [
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
        "\n",
        "# load the model\n",
        "model = load_model(\"/content/gdrive/My Drive/NLP_Projects/4/model.h5\")\n",
        "# evaluate model on training dataset\n",
        "_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
        "print('Train Accuracy: %f' % (acc*100))\n",
        "# evaluate model on test dataset\n",
        "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))\n",
        "\n",
        "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "# test negative text\n",
        "text1 = 'This is a bad movie. Do not watch it. It sucks.'\n",
        "percent, sentiment = predict_sentiment(text1, vocab, tokenizer, max_length, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text1, sentiment, percent*100))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy: 100.000000\n",
            "Test Accuracy: 85.500002\n",
            "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
            "Sentiment: POSITIVE (89.759%)\n",
            "Review: [This is a bad movie. Do not watch it. It sucks.]\n",
            "Sentiment: POSITIVE (88.574%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}