{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AOE-gZl4TLWa"
   },
   "source": [
    "# Generate sequences\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://8080-dot-11499948-dot-devshell.appspot.com/",
     "height": 202
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1960,
     "status": "ok",
     "timestamp": 1591558789455,
     "user": {
      "displayName": "Rachana Karekar",
      "photoUrl": "",
      "userId": "00733610830176035117"
     },
     "user_tz": -60
    },
    "id": "XO3mTxHcRon3",
    "outputId": "f7d1b3fd-b86c-42df-8bc9-0dd91e6ac3ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOK I.\n",
      "\n",
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
      "Artemis.); and also because I wanted to see in what\n",
      "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n",
      "Total Tokens: 118684\n",
      "Unique Tokens: 7409\n",
      "Total Sequences: 118633\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "  # open the file as read only\n",
    "  file = open(filename, 'r')\n",
    "  # read all text\n",
    "  text = file.read()\n",
    "  # close the file\n",
    "  file.close()\n",
    "  return text\n",
    "\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "  # replace '--' with a space ' '\n",
    "  doc = doc.replace('--', ' ')\n",
    "  # split into tokens by white space\n",
    "  tokens = doc.split()\n",
    "  # prepare regex for char filtering\n",
    "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "  # remove punctuation from each word\n",
    "  tokens = [re_punc.sub('', w) for w in tokens]\n",
    "  # remove remaining tokens that are not alphabetic\n",
    "  tokens = [word for word in tokens if word.isalpha()]\n",
    "  # make lower case\n",
    "  tokens = [word.lower() for word in tokens]\n",
    "  return tokens\n",
    "\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "  data = '\\n'.join(lines)\n",
    "  file = open(filename, 'w')\n",
    "  file.write(data)\n",
    "  file.close()\n",
    "\n",
    "\n",
    "# load document\n",
    "in_filename = \"/home/jupyter/NLP/Text_Generation/Plato.txt\"\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "  # select sequence of tokens\n",
    "  seq = tokens[i-length:i]\n",
    "  # convert into a line\n",
    "  line = ' '.join(seq)\n",
    "  # store\n",
    "  sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "# save sequences to file\n",
    "out_filename = \"/home/jupyter/NLP/Text_Generation/Plato_sequences.txt\"\n",
    "save_doc(sequences, out_filename)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0BBz5jxTQb3"
   },
   "source": [
    "# Train LSTM\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://8080-dot-11499948-dot-devshell.appspot.com/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 620392,
     "status": "error",
     "timestamp": 1591571949069,
     "user": {
      "displayName": "Rachana Karekar",
      "photoUrl": "",
      "userId": "00733610830176035117"
     },
     "user_tz": -60
    },
    "id": "4hf6KMEETTqU",
    "outputId": "2c5bdfdd-7f25-4593-998b-f0d1a6a79f48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            370500    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7410)              748410    \n",
      "=================================================================\n",
      "Total params: 1,269,810\n",
      "Trainable params: 1,269,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "118633/118633 [==============================] - 86s 725us/step - loss: 6.1348 - accuracy: 0.0751\n",
      "Epoch 2/100\n",
      "118633/118633 [==============================] - 85s 714us/step - loss: 5.6556 - accuracy: 0.1102\n",
      "Epoch 3/100\n",
      "118633/118633 [==============================] - 85s 717us/step - loss: 5.4102 - accuracy: 0.1331\n",
      "Epoch 4/100\n",
      "118633/118633 [==============================] - 84s 705us/step - loss: 5.2608 - accuracy: 0.1451\n",
      "Epoch 5/100\n",
      "118633/118633 [==============================] - 84s 706us/step - loss: 5.1464 - accuracy: 0.1557\n",
      "Epoch 6/100\n",
      "118633/118633 [==============================] - 82s 690us/step - loss: 5.0465 - accuracy: 0.1629\n",
      "Epoch 7/100\n",
      "118633/118633 [==============================] - 84s 707us/step - loss: 4.9548 - accuracy: 0.1684\n",
      "Epoch 8/100\n",
      "118633/118633 [==============================] - 85s 715us/step - loss: 4.8726 - accuracy: 0.1719\n",
      "Epoch 9/100\n",
      "118633/118633 [==============================] - 82s 694us/step - loss: 4.7942 - accuracy: 0.1759\n",
      "Epoch 10/100\n",
      "118633/118633 [==============================] - 84s 709us/step - loss: 4.7208 - accuracy: 0.1796\n",
      "Epoch 11/100\n",
      "118633/118633 [==============================] - 84s 708us/step - loss: 4.6504 - accuracy: 0.1830\n",
      "Epoch 12/100\n",
      "118633/118633 [==============================] - 83s 703us/step - loss: 4.5825 - accuracy: 0.1864\n",
      "Epoch 13/100\n",
      "118633/118633 [==============================] - 84s 704us/step - loss: 4.5174 - accuracy: 0.1903\n",
      "Epoch 14/100\n",
      "118633/118633 [==============================] - 85s 713us/step - loss: 4.4546 - accuracy: 0.1937\n",
      "Epoch 15/100\n",
      "118633/118633 [==============================] - 83s 699us/step - loss: 4.3953 - accuracy: 0.1972\n",
      "Epoch 16/100\n",
      "118633/118633 [==============================] - 84s 705us/step - loss: 4.3388 - accuracy: 0.1999\n",
      "Epoch 17/100\n",
      "118633/118633 [==============================] - 84s 705us/step - loss: 4.3726 - accuracy: 0.1971\n",
      "Epoch 18/100\n",
      "118633/118633 [==============================] - 83s 703us/step - loss: 4.3658 - accuracy: 0.1957\n",
      "Epoch 19/100\n",
      "118633/118633 [==============================] - 86s 722us/step - loss: 4.2380 - accuracy: 0.2045\n",
      "Epoch 20/100\n",
      "118633/118633 [==============================] - 84s 706us/step - loss: 4.1958 - accuracy: 0.2077\n",
      "Epoch 21/100\n",
      "118633/118633 [==============================] - 83s 696us/step - loss: 4.1250 - accuracy: 0.2127\n",
      "Epoch 22/100\n",
      "118633/118633 [==============================] - 83s 702us/step - loss: 4.0808 - accuracy: 0.2164\n",
      "Epoch 23/100\n",
      "118633/118633 [==============================] - 85s 718us/step - loss: 4.0456 - accuracy: 0.2191\n",
      "Epoch 24/100\n",
      "118633/118633 [==============================] - 83s 703us/step - loss: 4.0061 - accuracy: 0.2228\n",
      "Epoch 25/100\n",
      "118633/118633 [==============================] - 83s 697us/step - loss: 3.9676 - accuracy: 0.2257\n",
      "Epoch 26/100\n",
      "118633/118633 [==============================] - 84s 708us/step - loss: 3.9306 - accuracy: 0.2293\n",
      "Epoch 27/100\n",
      "118633/118633 [==============================] - 84s 709us/step - loss: 3.8986 - accuracy: 0.2314\n",
      "Epoch 28/100\n",
      "118633/118633 [==============================] - 83s 696us/step - loss: 3.8657 - accuracy: 0.2340\n",
      "Epoch 29/100\n",
      "118633/118633 [==============================] - 84s 705us/step - loss: 3.8352 - accuracy: 0.2379\n",
      "Epoch 30/100\n",
      "118633/118633 [==============================] - 83s 699us/step - loss: 3.8039 - accuracy: 0.2412\n",
      "Epoch 31/100\n",
      "118633/118633 [==============================] - 84s 706us/step - loss: 3.7744 - accuracy: 0.2437\n",
      "Epoch 32/100\n",
      "118633/118633 [==============================] - 83s 696us/step - loss: 3.7456 - accuracy: 0.2475\n",
      "Epoch 33/100\n",
      "118633/118633 [==============================] - 84s 706us/step - loss: 3.7187 - accuracy: 0.2505\n",
      "Epoch 34/100\n",
      "118633/118633 [==============================] - 84s 705us/step - loss: 3.6916 - accuracy: 0.2517\n",
      "Epoch 35/100\n",
      "118633/118633 [==============================] - 84s 704us/step - loss: 3.6652 - accuracy: 0.2563\n",
      "Epoch 36/100\n",
      "118633/118633 [==============================] - 85s 713us/step - loss: 3.6397 - accuracy: 0.2592\n",
      "Epoch 37/100\n",
      "118633/118633 [==============================] - 83s 701us/step - loss: 3.6146 - accuracy: 0.2623\n",
      "Epoch 38/100\n",
      "118633/118633 [==============================] - 84s 709us/step - loss: 3.5900 - accuracy: 0.2640\n",
      "Epoch 39/100\n",
      "118633/118633 [==============================] - 83s 697us/step - loss: 3.5651 - accuracy: 0.2675\n",
      "Epoch 40/100\n",
      "118633/118633 [==============================] - 83s 695us/step - loss: 3.5427 - accuracy: 0.2698\n",
      "Epoch 41/100\n",
      "118633/118633 [==============================] - 83s 701us/step - loss: 3.5188 - accuracy: 0.2738\n",
      "Epoch 42/100\n",
      "118633/118633 [==============================] - 84s 711us/step - loss: 3.4961 - accuracy: 0.2762\n",
      "Epoch 43/100\n",
      "118633/118633 [==============================] - 84s 706us/step - loss: 3.4747 - accuracy: 0.2780\n",
      "Epoch 44/100\n",
      "118633/118633 [==============================] - 85s 716us/step - loss: 3.4545 - accuracy: 0.2815\n",
      "Epoch 45/100\n",
      "118633/118633 [==============================] - 83s 700us/step - loss: 3.4310 - accuracy: 0.2858\n",
      "Epoch 46/100\n",
      "118633/118633 [==============================] - 83s 703us/step - loss: 3.4091 - accuracy: 0.2873\n",
      "Epoch 47/100\n",
      "118633/118633 [==============================] - 82s 689us/step - loss: 3.3919 - accuracy: 0.2899\n",
      "Epoch 48/100\n",
      "118633/118633 [==============================] - 83s 700us/step - loss: 3.3704 - accuracy: 0.2924\n",
      "Epoch 49/100\n",
      "118633/118633 [==============================] - 84s 704us/step - loss: 3.3505 - accuracy: 0.2961\n",
      "Epoch 50/100\n",
      "118633/118633 [==============================] - 82s 694us/step - loss: 3.3313 - accuracy: 0.2985\n",
      "Epoch 51/100\n",
      "118633/118633 [==============================] - 83s 700us/step - loss: 3.3122 - accuracy: 0.3002\n",
      "Epoch 52/100\n",
      "118633/118633 [==============================] - 83s 701us/step - loss: 3.2951 - accuracy: 0.3030\n",
      "Epoch 53/100\n",
      "118633/118633 [==============================] - 83s 702us/step - loss: 3.2744 - accuracy: 0.3060\n",
      "Epoch 54/100\n",
      "118633/118633 [==============================] - 83s 696us/step - loss: 3.2577 - accuracy: 0.3095\n",
      "Epoch 55/100\n",
      "118633/118633 [==============================] - 84s 705us/step - loss: 3.2378 - accuracy: 0.3118\n",
      "Epoch 56/100\n",
      "118633/118633 [==============================] - 84s 705us/step - loss: 3.2203 - accuracy: 0.3132\n",
      "Epoch 57/100\n",
      "118633/118633 [==============================] - 84s 709us/step - loss: 3.2012 - accuracy: 0.3180\n",
      "Epoch 58/100\n",
      "118633/118633 [==============================] - 83s 703us/step - loss: 3.1839 - accuracy: 0.3181\n",
      "Epoch 59/100\n",
      "118633/118633 [==============================] - 83s 696us/step - loss: 3.1654 - accuracy: 0.3210\n",
      "Epoch 60/100\n",
      "118633/118633 [==============================] - 83s 696us/step - loss: 3.1495 - accuracy: 0.3243\n",
      "Epoch 61/100\n",
      "118633/118633 [==============================] - 82s 691us/step - loss: 3.1341 - accuracy: 0.3264\n",
      "Epoch 62/100\n",
      "118633/118633 [==============================] - 83s 703us/step - loss: 3.1149 - accuracy: 0.3292\n",
      "Epoch 63/100\n",
      "118633/118633 [==============================] - 82s 695us/step - loss: 3.0989 - accuracy: 0.3317\n",
      "Epoch 64/100\n",
      "118633/118633 [==============================] - 83s 701us/step - loss: 3.0836 - accuracy: 0.3339\n",
      "Epoch 65/100\n",
      "118633/118633 [==============================] - 83s 698us/step - loss: 3.0634 - accuracy: 0.3374\n",
      "Epoch 66/100\n",
      "118633/118633 [==============================] - 83s 696us/step - loss: 3.0496 - accuracy: 0.3386\n",
      "Epoch 67/100\n",
      "118633/118633 [==============================] - 85s 717us/step - loss: 3.0330 - accuracy: 0.3425\n",
      "Epoch 68/100\n",
      "118633/118633 [==============================] - 84s 711us/step - loss: 3.0162 - accuracy: 0.3448\n",
      "Epoch 69/100\n",
      "118633/118633 [==============================] - 82s 695us/step - loss: 2.9994 - accuracy: 0.3476\n",
      "Epoch 70/100\n",
      "118633/118633 [==============================] - 82s 688us/step - loss: 2.9856 - accuracy: 0.3497\n",
      "Epoch 71/100\n",
      "118633/118633 [==============================] - 84s 708us/step - loss: 2.9676 - accuracy: 0.3526\n",
      "Epoch 72/100\n",
      "118633/118633 [==============================] - 83s 699us/step - loss: 2.9544 - accuracy: 0.3538\n",
      "Epoch 73/100\n",
      "118633/118633 [==============================] - 82s 692us/step - loss: 2.9380 - accuracy: 0.3566\n",
      "Epoch 74/100\n",
      "118633/118633 [==============================] - 82s 693us/step - loss: 2.9206 - accuracy: 0.3606\n",
      "Epoch 75/100\n",
      "118633/118633 [==============================] - 83s 701us/step - loss: 2.9080 - accuracy: 0.3618\n",
      "Epoch 76/100\n",
      "118633/118633 [==============================] - 85s 717us/step - loss: 2.8931 - accuracy: 0.3639\n",
      "Epoch 77/100\n",
      "118633/118633 [==============================] - 83s 702us/step - loss: 2.8774 - accuracy: 0.3671\n",
      "Epoch 78/100\n",
      "118633/118633 [==============================] - 84s 707us/step - loss: 2.8621 - accuracy: 0.3698\n",
      "Epoch 79/100\n",
      "118633/118633 [==============================] - 84s 711us/step - loss: 2.8479 - accuracy: 0.3723\n",
      "Epoch 80/100\n",
      "118633/118633 [==============================] - 84s 709us/step - loss: 2.8323 - accuracy: 0.3742\n",
      "Epoch 81/100\n",
      "118633/118633 [==============================] - 84s 706us/step - loss: 2.8169 - accuracy: 0.3773\n",
      "Epoch 82/100\n",
      "118633/118633 [==============================] - 83s 700us/step - loss: 2.8036 - accuracy: 0.3794\n",
      "Epoch 83/100\n",
      "118633/118633 [==============================] - 84s 704us/step - loss: 2.7891 - accuracy: 0.3815\n",
      "Epoch 84/100\n",
      "118633/118633 [==============================] - 83s 698us/step - loss: 2.7724 - accuracy: 0.3842\n",
      "Epoch 85/100\n",
      "118633/118633 [==============================] - 84s 706us/step - loss: 2.7582 - accuracy: 0.3869\n",
      "Epoch 86/100\n",
      "118633/118633 [==============================] - 84s 711us/step - loss: 2.7472 - accuracy: 0.3895\n",
      "Epoch 87/100\n",
      "118633/118633 [==============================] - 83s 698us/step - loss: 2.7316 - accuracy: 0.3915\n",
      "Epoch 88/100\n",
      "118633/118633 [==============================] - 84s 710us/step - loss: 2.7151 - accuracy: 0.3951\n",
      "Epoch 89/100\n",
      "118633/118633 [==============================] - 82s 692us/step - loss: 2.7026 - accuracy: 0.3976\n",
      "Epoch 90/100\n",
      "118633/118633 [==============================] - 83s 698us/step - loss: 2.6922 - accuracy: 0.3980\n",
      "Epoch 91/100\n",
      "118633/118633 [==============================] - 83s 698us/step - loss: 2.6764 - accuracy: 0.4010\n",
      "Epoch 92/100\n",
      "118633/118633 [==============================] - 83s 704us/step - loss: 2.6626 - accuracy: 0.4034\n",
      "Epoch 93/100\n",
      "118633/118633 [==============================] - 83s 702us/step - loss: 2.6523 - accuracy: 0.4048\n",
      "Epoch 94/100\n",
      "118633/118633 [==============================] - 84s 711us/step - loss: 2.6343 - accuracy: 0.4083\n",
      "Epoch 95/100\n",
      "118633/118633 [==============================] - 83s 702us/step - loss: 2.6209 - accuracy: 0.4119\n",
      "Epoch 96/100\n",
      "118633/118633 [==============================] - 85s 716us/step - loss: 2.6085 - accuracy: 0.4128\n",
      "Epoch 97/100\n",
      "118633/118633 [==============================] - 84s 711us/step - loss: 2.5953 - accuracy: 0.4154\n",
      "Epoch 98/100\n",
      "118633/118633 [==============================] - 84s 711us/step - loss: 2.5838 - accuracy: 0.4194\n",
      "Epoch 99/100\n",
      "118633/118633 [==============================] - 84s 706us/step - loss: 2.5693 - accuracy: 0.4192\n",
      "Epoch 100/100\n",
      "118633/118633 [==============================] - 83s 703us/step - loss: 2.5535 - accuracy: 0.4234\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# define the model\n",
    "def define_model(vocab_size, seq_length):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "  model.add(LSTM(100, return_sequences=True))\n",
    "  model.add(LSTM(100))\n",
    "  model.add(Dense(100, activation='relu'))\n",
    "  model.add(Dense(vocab_size, activation='softmax'))\n",
    "  # compile network\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  # summarize defined model\n",
    "  model.summary()\n",
    "  plot_model(model, to_file='model.png', show_shapes=True)\n",
    "  return model\n",
    "\n",
    "\n",
    "# load\n",
    "in_filename = \"/home/jupyter/NLP/Text_Generation/Plato_sequences.txt\"\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "# define model\n",
    "model = define_model(vocab_size, seq_length)\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    "# save the model to file\n",
    "model.save(\"/home/jupyter/NLP/Text_Generation/model.h5\")\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open(\"/home/jupyter/NLP/Text_Generation/tokenizer.pkl\", 'wb'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he said that was surely an extraordinary drink to be given to a person in his condition not so extraordinary i replied if you bear in mind that in former days as is commonly said before the time of herodicus the guild of asclepius did not practise our present system of\n",
      "\n",
      "state\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "    # append to input\n",
    "    in_text += ' ' + out_word\n",
    "    result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = \"/home/jupyter/NLP/Text_Generation/Plato_sequences.txt\"\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "# load the model\n",
    "model = load_model(\"/home/jupyter/NLP/Text_Generation/model.h5\")\n",
    "# load the tokenizer\n",
    "tokenizer = load(open(\"/home/jupyter/NLP/Text_Generation/tokenizer.pkl\", 'rb'))\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPJy+N/RMEUN7js8OSVH43g",
   "name": "Text_Generation.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
