{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoYP9C5dq9_V",
        "colab_type": "text"
      },
      "source": [
        "## Data used: http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
        "\n",
        "---\n",
        "The\n",
        "dataset is comprised of 1,000 positive and 1,000 negative movie reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps6T-k80lNZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_EH6LCrvdp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiabd_vGkcmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from pandas import DataFrame\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1mOMNg_9nYG",
        "colab_type": "text"
      },
      "source": [
        "# Store the Vocabulary\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bumZefyklLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "\n",
        "  # close the file\n",
        "  file.close()\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "\n",
        "  return tokens \n",
        "\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\n",
        "  # load doc\n",
        "  doc = load_doc(filename)\n",
        "\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "\n",
        "  # update counts\n",
        "  vocab.update(tokens)  \n",
        "\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "    if not filename.endswith(\".txt\"):\n",
        "      next\n",
        "\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "\n",
        "    # add doc to vocab\n",
        "    add_doc_to_vocab(path, vocab) \n",
        "\n",
        "\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename, 'w')\n",
        "  file.write(data)  \n",
        "  file.close()   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvhVMACduEYK",
        "colab_type": "code",
        "outputId": "06e5ceba-b3d9-4501-c057-b5ba978ffc55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "# define vocab\n",
        "vocab = Counter()\n",
        "\n",
        "# add all docs to vocab\n",
        "process_docs(\"/content/gdrive/My Drive/NLP_Projects/4/txt_sentoken/neg\", vocab)\n",
        "process_docs(\"/content/gdrive/My Drive/NLP_Projects/4/txt_sentoken/pos\", vocab)\n",
        "\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))\n",
        "\n",
        "# keep tokens with > 2 occurrence\n",
        "min_occurence = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurence]\n",
        "print(len(tokens))\n",
        "\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, \"/content/gdrive/My Drive/NLP_Projects/4/vocab.txt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46557\n",
            "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n",
            "27139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqUK9QhZkpT6",
        "colab_type": "text"
      },
      "source": [
        "# Splitting the data as train and test\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MvARAoA9CSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "  # load the doc\n",
        "  doc = load_doc(filename)\n",
        "\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "\n",
        "  # filter by vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  lines = list()\n",
        "\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "\n",
        "    # skip any reviews in the test set\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load and clean the doc\n",
        "    line = doc_to_line(path, vocab)\n",
        "    # add to list\n",
        "    lines.append(line)\n",
        "\n",
        "  return lines\n",
        "\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "  # load documents\n",
        "  neg = process_docs(\"/content/gdrive/My Drive/NLP_Projects/4/txt_sentoken/neg\", vocab, is_train)\n",
        "  pos = process_docs(\"/content/gdrive/My Drive/NLP_Projects/4/txt_sentoken/pos\", vocab, is_train)\n",
        "  docs = neg + pos\n",
        "\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "\n",
        "  return docs, labels\n",
        "\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yWeGmgJBpSG",
        "colab_type": "code",
        "outputId": "661e9308-c858-4ac2-e9a1-eb1f80be4917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# load the vocabulary\n",
        "vocab_filename = \"/content/gdrive/My Drive/NLP_Projects/4/vocab.txt\"\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "print(Xtrain.shape, Xtest.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1800, 26897) (200, 26897)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX7INgGzk2OS",
        "colab_type": "text"
      },
      "source": [
        "# MLP for Sentiment Analysis\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPPCH7IilEiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the model\n",
        "def define_model(n_words):\n",
        "  # define network\n",
        "  model = Sequential()\n",
        "  model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # compile network\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# evaluate a neural network model\n",
        "def evaluate_model(Xtrain, ytrain, Xtest, ytest):\n",
        "  scores = list()\n",
        "  n_repeats = 10\n",
        "  n_words = Xtest.shape[1]\n",
        "  for i in range(n_repeats):\n",
        "    # define network\n",
        "    model = define_model(n_words)\n",
        "\n",
        "    # fit network\n",
        "    model.fit(Xtrain, ytrain, epochs=10, verbose=0)\n",
        "\n",
        "    # evaluate\n",
        "    _, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "    scores.append(acc)\n",
        "    print('%d accuracy: %s' % ((i+1), acc))\n",
        "\n",
        "  return scores\n",
        "\n",
        "\n",
        "# prepare bag of words encoding of docs\n",
        "def prepare_data(train_docs, test_docs, mode):\n",
        "  # create the tokenizer\n",
        "  tokenizer = Tokenizer()\n",
        "\n",
        "  # fit the tokenizer on the documents\n",
        "  tokenizer.fit_on_texts(train_docs)\n",
        "\n",
        "  # encode training data set\n",
        "  Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
        "\n",
        "  # encode training data set\n",
        "  Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
        "\n",
        "  return Xtrain, Xtest  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apyL6AF2lbY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "687818ca-a761-4819-96f9-9e9b1bfb61d9"
      },
      "source": [
        "# run experiment\n",
        "modes = ['binary', 'count', 'tfidf', 'freq']\n",
        "results = DataFrame()\n",
        "for mode in modes:\n",
        "  # prepare data for mode\n",
        "  Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
        "\n",
        "  # evaluate model on data for mode\n",
        "  results[mode] = evaluate_model(Xtrain, ytrain, Xtest, ytest)\n",
        "\n",
        "# summarize results\n",
        "print(results.describe())\n",
        "\n",
        "# plot results\n",
        "results.boxplot()\n",
        "pyplot.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 accuracy: 0.9200000166893005\n",
            "2 accuracy: 0.9200000166893005\n",
            "3 accuracy: 0.9300000071525574\n",
            "4 accuracy: 0.9100000262260437\n",
            "5 accuracy: 0.9200000166893005\n",
            "6 accuracy: 0.9200000166893005\n",
            "7 accuracy: 0.9150000214576721\n",
            "8 accuracy: 0.9200000166893005\n",
            "9 accuracy: 0.9200000166893005\n",
            "10 accuracy: 0.9300000071525574\n",
            "1 accuracy: 0.8949999809265137\n",
            "2 accuracy: 0.8999999761581421\n",
            "3 accuracy: 0.8899999856948853\n",
            "4 accuracy: 0.8700000047683716\n",
            "5 accuracy: 0.8999999761581421\n",
            "6 accuracy: 0.8949999809265137\n",
            "7 accuracy: 0.9049999713897705\n",
            "8 accuracy: 0.8799999952316284\n",
            "9 accuracy: 0.8949999809265137\n",
            "10 accuracy: 0.8999999761581421\n",
            "1 accuracy: 0.8450000286102295\n",
            "2 accuracy: 0.8899999856948853\n",
            "3 accuracy: 0.8550000190734863\n",
            "4 accuracy: 0.8799999952316284\n",
            "5 accuracy: 0.8799999952316284\n",
            "6 accuracy: 0.8600000143051147\n",
            "7 accuracy: 0.8650000095367432\n",
            "8 accuracy: 0.8849999904632568\n",
            "9 accuracy: 0.8700000047683716\n",
            "10 accuracy: 0.8550000190734863\n",
            "1 accuracy: 0.8600000143051147\n",
            "2 accuracy: 0.8700000047683716\n",
            "3 accuracy: 0.8650000095367432\n",
            "4 accuracy: 0.8700000047683716\n",
            "5 accuracy: 0.875\n",
            "6 accuracy: 0.8700000047683716\n",
            "7 accuracy: 0.8650000095367432\n",
            "8 accuracy: 0.8650000095367432\n",
            "9 accuracy: 0.8700000047683716\n",
            "10 accuracy: 0.875\n",
            "          binary      count      tfidf       freq\n",
            "count  10.000000  10.000000  10.000000  10.000000\n",
            "mean    0.920500   0.893000   0.868500   0.868500\n",
            "std     0.005986   0.010593   0.014916   0.004743\n",
            "min     0.910000   0.870000   0.845000   0.860000\n",
            "25%     0.920000   0.891250   0.856250   0.865000\n",
            "50%     0.920000   0.895000   0.867500   0.870000\n",
            "75%     0.920000   0.900000   0.880000   0.870000\n",
            "max     0.930000   0.905000   0.890000   0.875000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7PisVSRptIF",
        "colab_type": "text"
      },
      "source": [
        "# Predicting Sentiment for New Reviews\n",
        "\n",
        "---\n",
        "\n",
        "Considering above results, it can be seen that 'binary' mode gave the best accuracy value. Hence we will use this mode for predicting on a new review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRId4bt-pvn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, model):\n",
        "  # clean\n",
        "  tokens = clean_doc(review)\n",
        "\n",
        "  # filter by vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "\n",
        "  # convert to line\n",
        "  line = ' '.join(tokens)\n",
        "\n",
        "  # encode\n",
        "  encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
        "\n",
        "  # predict sentiment\n",
        "  yhat = model.predict(encoded, verbose=0)\n",
        "\n",
        "  # retrieve predicted percentage and label\n",
        "  percent_pos = yhat[0,0]\n",
        "  if round(percent_pos) == 0:\n",
        "    return (1-percent_pos), 'NEGATIVE'\n",
        "  return percent_pos, 'POSITIVE'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEmy2EUkqdS_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "5fd4b7d0-d691-4683-bf75-2fb45378a8ed"
      },
      "source": [
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
        "\n",
        "# define network\n",
        "n_words = Xtrain.shape[1]\n",
        "model = define_model(n_words)\n",
        "\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " - 1s - loss: 0.4687 - accuracy: 0.7800\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.0563 - accuracy: 0.9944\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0149 - accuracy: 1.0000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 8.4596e-04 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 5.8451e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 4.2789e-04 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7ff444e901d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h8d28Tqqv_m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "7ee4ee40-aa5f-4dc9-8cfc-3d6c203f8bca"
      },
      "source": [
        "\n",
        "# test positive text\n",
        "text = 'Best movie ever! It was great, I recommend it.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "\n",
        "# test negative text\n",
        "text = 'This is a bad movie.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: [Best movie ever! It was great, I recommend it.]\n",
            "Sentiment: POSITIVE (58.036%)\n",
            "Review: [This is a bad movie.]\n",
            "Sentiment: NEGATIVE (64.994%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}